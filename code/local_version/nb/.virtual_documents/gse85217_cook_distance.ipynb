


!jupyter-lab enable widgetsnbextension





# lib
#import modin.pandas as pd
import pandas as pd
import numpy as np
import os
from collections import OrderedDict
import umap

# fig
import matplotlib.pyplot as plt
import seaborn as sns

# local lib
import sys
sys.path.insert(1,'/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/code/local_version/fun')

from parser import Data


path_data='/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/data/in/'
path_exp_mat = path_data + 'GSE85217_M_exp_763_MB_SubtypeStudy_TaylorLab_parsed.txt'
path_meta = path_data + 'GSE85217_Cavalli_subgroups_information_parsed.csv'

data=Data()
data.add_exp_mat(path_exp_mat,index_col="genes_name")
data.add_meta(path_meta=path_meta,index_col="samples_name")


from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import statsmodels.formula.api as smf


def get_design_matrix(metadata,colname,**kwargs):
    return pd.get_dummies(metadata[colname],dtype=int,**kwargs)


design=get_design_matrix(data.meta,colname="Subgroup")


data.meta["Subgroup"].value_counts()


data_nb=pd.concat([data.exp_mat.loc['TSPAN6',data.meta.index.to_list()],data.meta['Subtype']],axis=1)





def trimmed_mean(x: np.ndarray, trim: float = 0.1, **kwargs) -> np.ndarray:

    assert trim <= 0.5

    kwargs.setdefault('axis',0)
    
    axis = kwargs.get("axis")
    s = np.sort(x,**kwargs)
    n = x.shape[axis]
    ntrim = math.floor(n * trim)
    return np.take(s, np.arange(ntrim, n - ntrim), axis).mean(**kwargs) 


import math

def trimmed_design_variance(exp_mat: np.ndarray, design_series: pd.Series) -> np.ndarray:

    # how much to trim at different n
    trimratio = (1 / 3, 1 / 4, 1 / 8)

    def trimfn(x:float)-> int :
        return 2 if x >= 23.5 else 1 if x >= 3.5 else 0

    ns = design_series.value_counts()
    sqerror = np.zeros_like(exp_mat)
    design_vars = design_series.unique()
    
    for var in design_vars:
        var_means = trimmed_mean(x=exp_mat[design_series == var,:],trim=trimratio[trimfn(ns[var])],axis=0)
        sqerror[design_series == var,:] = exp_mat[design_series == var,:] - var_means

    sqerror**=2

    variance_estimated = np.zeros((len(ns), exp_mat.shape[1]), dtype=float)
    for i, var in enumerate(design_vars):
        scale = [2.04, 1.86, 1.51][trimfn(ns[var])]
        variance_means = trimmed_mean(sqerror[design_series == var, :], trim=trimratio[trimfn(ns[var])], axis=0)
        variance_estimated[i,:] = scale * variance_means
    
    return variance_estimated.max(axis=0)


trimmed_design_variance(exp_mat=np.array(data.exp_mat.T), design_series=data.meta['Subtype'])


def robust_method_of_moment_disp(exp_mat:np.ndarray,design_series:pd.Series):

    v = trimmed_design_variance(exp_mat,design_series)

    m = exp_mat.mean(axis=0)
    alpha = (v - m) / m**2

    # cannot use the typical min_disp = 1e-8 here or else all counts in the same
    # group as the outlier count will get an extreme Cook's distance
    min_disp = 0.04
    np.maximum(alpha, min_disp, out=alpha)
    return alpha


alpha = robust_method_of_moment_disp(exp_mat=np.array(data.exp_mat.T), design_series=data.meta['Subtype'])


len(alpha)





# Calculate Cook's Distance
def calculate_cooks_distance(model):
    # Residuals and leverage
    infl = model.get_influence()
    cooks_d = infl.cooks_distance[0]
    return cooks_d

def calculate_distance_cook(exp_mat:pd.DataFrame,design_series:pd.Series):

    outliers={}

    alphas = robust_method_of_moment_disp(np.array(exp_mat), design_series)
    
    genes = exp_mat.columns
    exp_mat[design_series.name]=design_series
    
    m = len(design_series)
    p = len(design_series.unique()) + 1
    f_cutoff = f.ppf(0.99, p, m - p)
    
    for i,gene in enumerate(genes):
        formula = f"{gene} ~ C({design_series.name})"
        model = smf.glm(formula=formula, data=exp_mat[[gene,design_series.name]], family=sm.families.NegativeBinomial(alpha=1//alphas[i])).fit()

        infl = model.get_influence()
        cooks_d = infl.cooks_distance[0]

        outliers_index = np.where(cooks_d > f_cutoff)[0]
        
        if len(outliers_index) > 0 :
            outliers[gene]=tuple(outliers_index,cooks_d)

        return outliers    


a = np.array([[0, 1, 2],

              [0, 2, 4],

              [0, 3, 6]])


calculate_distance_cook(exp_mat=data.exp_mat.T,design_series=data.meta['Subtype'])


data.meta['Subtype'].unique()


test=data.exp_mat.columns.to_list()


test=test.remove('MB_SubtypeStudy_55001')


print(test)


#' + '.join(design.columns)


def generator_formula(exp_mat:pd.DataFrame,design_matrix:pd.DataFrame):

    groups = ' + '.join(design_matrix.columns)
    for gene in exp_mat.columns:
        yield f'{gene} ~ ' + groups


gen = generator_formula(exp_mat=data.exp_mat.T,design_matrix=design)


import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import f


df=pd.concat([data.exp_mat.T,design],axis=1)


design


model_nb = smf.glm("TSPAN6 ~ Group3 + Group4 + SHH + WNT +0", data=df, family=sm.families.NegativeBinomial(alpha=0.07)).fit()


model_nb.summary2()


plt.figure(figsize=(10, 6))
# KDE plot for predicted values
plt.plot(model_nb.predict(), np.array(data.exp_mat)[0],'.')


len(model_nb.predict())





counts=np.array(data.exp_mat)


counts=counts[]





m = design.shape[0]
p = design.shape[1]


f_cutoff = f.ppf(0.99, p, m - p)  # Seuil de Cook Ã  99%

# Identification des outliers potentiels
outliers_nb = np.where(cooks_d_nb > f_cutoff)[0]
print("Indices des valeurs aberrantes potentielles:", outliers_nb)
print("Distances de Cook des valeurs aberrantes:", cooks_d_nb[outliers_nb])
print("Seuil de la distance de Cook (F-distribution):", f_cutoff)


import numpy as np
from scipy import stats
import statsmodels.api as sm

# generate some data to check
nobs = 1000
n, p = 50, 0.25
dist0 = stats.nbinom(n, p)
y = dist0.rvs(size=nobs)
x = np.ones(nobs)

loglike_method = 'nb1'  # or use 'nb2'
res = sm.NegativeBinomial(y, x, loglike_method=loglike_method).fit(start_params=[0.1, 0.1])

print dist0.mean()
print res.params

mu = res.predict()   # use this for mean if not constant
mu = np.exp(res.params[0])   # shortcut, we just regress on a constant
alpha = res.params[1]

if loglike_method == 'nb1':
    Q = 1
elif loglike_method == 'nb2':    
    Q = 0

size = 1. / alpha * mu**Q
prob = size / (size + mu)

print 'data generating parameters', n, p
print 'estimated params          ', size, prob

#estimated distribution
dist_est = stats.nbinom(size, prob)


import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import statsmodels.formula.api as smf 
import statsmodels.api as sm

# creating the dataframe
black = [119,16,12,7,3,2,0]
white = [1070,60,14,4,0,0,1]
resp = np.hstack([np.repeat(range(7), black), np.repeat(range(7), white)])
race = np.repeat(['black', 'white'], [sum(black), sum(white)])
victim = pd.DataFrame(data={'resp': resp, 'race': race})

# performing the parameter sweep
n_samples = 500
alphas = np.linspace(0, 10, n_samples)
llf = np.full(n_samples, fill_value=np.nan)
for i, alpha in enumerate(alphas):
    try:
        model = smf.glm(formula = "resp ~ C(race)", data=victim, family=sm.families.NegativeBinomial(alpha=alpha)).fit()
    except:
        continue
    llf[i] = model.llf

# record the optimal value
alpha_opt = alphas[np.nanargmax(llf)]
dispersion = 1 / alpha_opt

# plotting the results
plt.plot(alphas, llf, label='Log-liklihood')
plt.axvline(x=alpha_opt, color='g', label=f'Dispersion: {dispersion:0.5f}')
plt.legend()


race


victim



