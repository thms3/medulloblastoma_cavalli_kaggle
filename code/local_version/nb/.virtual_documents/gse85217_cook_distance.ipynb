


!jupyter-lab enable widgetsnbextension





# lib
#import modin.pandas as pd
import pandas as pd
import numpy as np
import os
from collections import OrderedDict
import umap

# fig
import matplotlib.pyplot as plt
import seaborn as sns

# local lib
import sys
sys.path.insert(1,'/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/code/local_version/fun')

from parser import Data


path_data='/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/data/in/'
path_exp_mat = path_data + 'GSE85217_M_exp_763_MB_SubtypeStudy_TaylorLab_parsed.txt'
path_meta = path_data + 'GSE85217_Cavalli_subgroups_information_parsed.csv'

data=Data()
data.add_exp_mat(path_exp_mat,index_col="genes_name")
data.add_meta(path_meta=path_meta,index_col="samples_name")


from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import statsmodels.formula.api as smf


def get_design_matrix(metadata,colname,**kwargs):
    return pd.get_dummies(metadata[colname],dtype=int,**kwargs)


design=get_design_matrix(data.meta,colname="Subtype")


data.meta["Subtype"].value_counts()


data_nb=pd.concat([data.exp_mat.loc['TSPAN6',data.meta.index.to_list()],data.meta['Subtype']],axis=1)


nb_model=smf.glm('TSPAN6 ~ Subtype',data=data_nb,family=sm.families.NegativeBinomial()).fit()





import math

def trimmed_mean(x: np.ndarray, trim: float = 0.1):

    assert trim <= 0.5

    assert trim <= 0.5
    if "axis" in kwargs:
        axis = kwargs.get("axis")
        s = np.sort(x, axis=axis)
        n = x.shape[axis]
        ntrim = floor(n * trim)
        return np.take(s, np.arange(ntrim, n - ntrim), axis).mean(axis)
    else:
        n = len(x)
        s = np.sort(x)
        ntrim = math.floor(n * trim)
        return s[ntrim : n - ntrim].mean()

def trimmed_variance(x:np.ndarray, trim:float = 0.125, axis:int = 0):

    rm = trimmed_mean(x, trim=trim)
    sqerror = (x - rm) ** 2
    return 1.51 * trimmed_mean(sqerror, trim=trim)

def robust_method_of_moments_disp(norm_counts:  np.ndarray, **kwargs):

    v = trimmed_mean(norm_counts, **kwargs)

    m = norm_counts.mean()
    alpha = (v - m) / m**2
    min_disp=0.04
    np.maximum(alpha, min_disp, out=alpha)
    return alpha

def trimmed_group_variance(counts:np.ndarray, design:pd.DataFrame):

    trimratio=(1/3,1/4,1/8)

    def trimfn(x:float)-> int :
        return 2 if x >= 23.5 else 1 if x >= 3.5 else 0

    group_ratio=np.array([trimratio[trimfn(x)] for x in design.sum(axis=0)])

    group_means=pd.DataFrame(data=np.zeros((design.shape[0],1)),index=design.index.to_list())

    def get_index(design:pd.DataFrame,colname:str,value:int=1):
        return design.index[design[colname]==value].to_list()

    for i,colname in enumerate(design.columns):
        samples_group=get_index(design,colname)
        
        #print(samples_group)
        #group_means.loc[samples_group,]=1
        #group_means[samples_group,]=trimmed_mean(x=counts.loc[samples_group])

        

    
    #design_ratio = design * group_ratio

    #exp_sum=np.dot(counts,design_ratio.sum(axis=1))

    return group_means


import math

def trimmed_mean(x: np.ndarray, trim: float = 0.1,**kwargs):

    assert trim <= 0.5

    assert trim <= 0.5
    if "axis" in kwargs:
        axis = kwargs.get("axis")
        s = np.sort(x, axis=axis)
        n = x.shape[axis]
        ntrim = math.floor(n * trim)
        return np.take(s, np.arange(ntrim, n - ntrim), axis).mean(axis)
    else:
        n = len(x)
        s = np.sort(x)
        ntrim = math.floor(n * trim)
        return s[ntrim : n - ntrim].mean()


def trimmed_cell_variance(counts: np.ndarray, cells: pd.Series) -> np.ndarray:
    """Return trimmed variance of counts according to condition.

    Compute the variance after trimming data of its smallest and largest elements,
    grouped by cohorts, and return the max across cohorts.
    The trim factor is a function of data size.

    Parameters
    ----------
    counts : ndarray
        Sample-wise gene counts.

    cells : pandas.Series
        Cohort affiliation of each sample.

    Returns
    -------
    ndarray :
        Gene-wise trimmed variance estimate.
    """
    # how much to trim at different n
    trimratio = (1 / 3, 1 / 4, 1 / 8)
    # returns an index for the vector above for three sample size bins

    def trimfn(x: float) -> int:
        return 2 if x >= 23.5 else 1 if x >= 3.5 else 0

    ns = cells.value_counts()
    sqerror = np.zeros_like(counts)

    for lvl in cells.unique():
        cell_means = np.array(trimmed_mean(counts[cells == lvl, :], trim=trimratio[trimfn(ns[lvl])], axis=0))
        sqerror[cells == lvl, :] = counts[cells == lvl, :] - cell_means[None, :]

    sqerror **= 2

    varEst = np.zeros((len(ns), counts.shape[1]), dtype=float)
    for i, lvl in enumerate(cells.unique()):
        scale = [2.04, 1.86, 1.51][trimfn(ns[lvl])]
        varEst[i, :] = scale * trimmed_mean(
            sqerror[cells == lvl, :], trim=trimratio[trimfn(ns[lvl])], axis=0
        )

    return varEst.max(axis=0)


#group_ratio=trimmed_group_variance(counts=data.exp_mat.loc['TSPAN6'],design=design)
#group_ratio
#data.exp_mat.loc['TSPAN6',get_index(design,colname='Group3_alpha']
#trimmed_mean(x=data.exp_mat.loc['TSPAN6',get_index(design,colname='Group3_alpha')])


norm=np.array(data.exp_mat.T)


v=trimmed_cell_variance(counts=np.array(data.exp_mat.T),cells=data.meta['Subtype'])


m = norm.mean(0)


alpha = (v - m) / m**2


minDisp = 0.04


np.maximum(alpha, minDisp, out=alpha)





design


df=pd.concat([data.exp_mat.T,design],axis=1)


import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import f



model_nb = smf.glm("TSPAN6 ~ Group3_alpha+Group3_beta+Group3_gamma+Group4_alpha+Group4_beta+Group4_gamma+SHH_alpha+SHH_beta+SHH_delta+SHH_gamma+WNT_alpha+WNT_beta", data=df, family=sm.families.NegativeBinomial(alpha=0.04)).fit()


#Calcul de la distance de Cook pour chaque observation
influence_nb = model_nb.get_influence()
cooks_d_nb = influence_nb.cooks_distance[0]

# Définir le seuil de la loi F
m = design.shape[0]  # Nombre total d'échantillons
p = len(df.columns)  # Nombre total de paramètres (intercept + groupes)
f_cutoff = f.ppf(0.99, p, m - p)  # Seuil de Cook à 99%

# Identification des outliers potentiels
outliers_nb = np.where(cooks_d_nb > f_cutoff)[0]
print("Indices des valeurs aberrantes potentielles:", outliers_nb)
print("Distances de Cook des valeurs aberrantes:", cooks_d_nb[outliers_nb])
print("Seuil de la distance de Cook (F-distribution):", f_cutoff)



design.columns


data.exp_mat


def get_index(design:pd.DataFrame,colname:str,value:int=1):
        return design.index[design[colname]==value].to_list()


counts=data.exp_mat


count_trim=pd.DataFrame(np.zeros_like(counts),index=counts.index.to_list(),columns=counts.columns)


group_series=data.meta['Subtype']
[i for i,j in enumerate(list(group_series=='SHH_beta')) if j]


group_series


def trimmed_cell_variance(counts: np.ndarray, cells: pd.Series) -> np.ndarray:
    """Return trimmed variance of counts according to condition.

    Compute the variance after trimming data of its smallest and largest elements,
    grouped by cohorts, and return the max across cohorts.
    The trim factor is a function of data size.

    Parameters
    ----------
    counts : ndarray
        Sample-wise gene counts.

    cells : pandas.Series
        Cohort affiliation of each sample.

    Returns
    -------
    ndarray :
        Gene-wise trimmed variance estimate.
    """
    # how much to trim at different n
    trimratio = (1 / 3, 1 / 4, 1 / 8)
    # returns an index for the vector above for three sample size bins

    def trimfn(x: float) -> int:
        return 2 if x >= 23.5 else 1 if x >= 3.5 else 0

    ns = cells.value_counts()
    sqerror = np.zeros_like(counts)

    for lvl in cells.unique():
        cell_means = cast(
            np.ndarray,
            trimmed_mean(
                counts[cells == lvl, :], trim=trimratio[trimfn(ns[lvl])], axis=0
            ),
        )
        sqerror[cells == lvl, :] = counts[cells == lvl, :] - cell_means[None, :]

    sqerror **= 2

    varEst = np.zeros((len(ns), counts.shape[1]), dtype=float)
    for i, lvl in enumerate(cells.unique()):
        scale = [2.04, 1.86, 1.51][trimfn(ns[lvl])]
        varEst[i, :] = scale * trimmed_mean(
            sqerror[cells == lvl, :], trim=trimratio[trimfn(ns[lvl])], axis=0
        )

    return varEst.max(axis=0)



def trimmed_variance(
    x: np.ndarray, trim: float = 0.125, axis: int = 0
) -> Union[float, np.ndarray]:
    """Return trimmed variance.

    Compute the variance after trimming data of its smallest and largest quantiles.

    Parameters
    ----------
    features : ndarray
        Data whose trimmed variance to compute.

    trim : float
        Fraction of data to trim at each end. (default: ``0.125``).

    axis : int
        Dimension along which to compute variance. (default: ``0``).

    Returns
    -------
    float or ndarray
        Trimmed variances.
    """
    rm = trimmed_mean(x, trim=trim, axis=axis)
    sqerror = (x - rm) ** 2
    # scale due to trimming of large squares
    return 1.51 * trimmed_mean(sqerror, trim=trim, axis=axis)


def trimmed_mean(x: np.ndarray, trim: float = 0.1, **kwargs) -> Union[float, np.ndarray]:
    """Return trimmed mean.

    Compute the mean after trimming data of its smallest and largest quantiles.

    Parameters
    ----------
    x : ndarray
        Data whose mean to compute.

    trim : float
        Fraction of data to trim at each end. (default: ``0.1``).

    **kwargs
        Keyword arguments, useful to pass axis.

    Returns
    -------
    float or ndarray :
        Trimmed mean.
    """
    assert trim <= 0.5
    if "axis" in kwargs:
        axis = kwargs.get("axis")
        s = np.sort(x, axis=axis)
        n = x.shape[axis]
        ntrim = floor(n * trim)
        return np.take(s, np.arange(ntrim, n - ntrim), axis).mean(axis)
    else:
        n = len(x)
        s = np.sort(x)
        ntrim = floor(n * trim)
        return s[ntrim : n - ntrim].mean()




def robust_method_of_moments_disp(
    normed_counts: np.ndarray, design_matrix: pd.DataFrame
) -> np.ndarray:
    """Perform dispersion estimation using a method of trimmed moments.

    Used for outlier detection based on Cook's distance.

    Parameters
    ----------
    normed_counts : ndarray
        Array of deseq2-normalized read counts. Rows: samples, columns: genes.

    design_matrix : pandas.DataFrame
        A DataFrame with experiment design information (to split cohorts).
        Indexed by sample barcodes. Unexpanded, *with* intercept.

    Returns
    -------
    ndarray
        Trimmed method of moment dispersion estimates.
        Used for outlier detection based on Cook's distance.
    """
    # if there are 3 or more replicates in any cell
    three_or_more = n_or_more_replicates(design_matrix, 3)
    if three_or_more.any():
        # 1 - group rows by unique combinations of design factors
        # 2 - keep only groups with 3 or more replicates
        # 3 - filter the counts matrix to only keep rows in those groups
        filtered_counts = normed_counts[three_or_more.values, :]
        filtered_design = design_matrix.loc[three_or_more, :]
        cell_id = pd.Series(
            filtered_design.groupby(
                filtered_design.columns.values.tolist()
            ).grouper.group_info[0],
            index=filtered_design.index,
        )
        v = trimmed_cell_variance(filtered_counts, cell_id)
    else:
        v = trimmed_variance(normed_counts)

    m = normed_counts.mean(0)
    alpha = (v - m) / m**2
    # cannot use the typical min_disp = 1e-8 here or else all counts in the same
    # group as the outlier count will get an extreme Cook's distance
    minDisp = 0.04
    np.maximum(alpha, minDisp, out=alpha)
    return alpha




