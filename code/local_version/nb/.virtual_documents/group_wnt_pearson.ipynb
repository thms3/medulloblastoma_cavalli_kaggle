


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler

from scipy.special import betainc

from concurrent.futures import ProcessPoolExecutor, as_completed


test = False


path_data = '/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/data/in/protein_coding/'
path_metadata = path_data + 'GSE85217_Cavalli_subgroups_information_protein_coding.csv'
path_expr_mat = path_data + 'GSE85217_M_exp_763_MB_SubtypeStudy_TaylorLab_protein_coding.csv'


metadata = pd.read_csv(path_metadata,index_col=0)
expr_mat = pd.read_csv(path_expr_mat,index_col=0)


expr_mat_wnt = expr_mat.loc[metadata[metadata['Subgroup']=='WNT']['Study_ID'].to_list(),:]


# order
expr_mat_order_vars = pd.concat([expr_mat.mean(axis=0),expr_mat.median(axis=0),expr_mat.std(axis=0)],axis=1)
expr_mat_order_vars.columns = ['mean','median','sd']
expr_mat_order_vars.sort_values(by=['mean','median','sd'],inplace=True)

expr_mat_wnt=expr_mat_wnt[expr_mat_order_vars.index.tolist()]


# standard scaler
sc = StandardScaler()
expr_mat_wnt_sc = sc.fit_transform(expr_mat_wnt)


import threading
import psutil
import time

class ResourceMonitor:
    def __init__(self,t_sleep=1):
        # Event to control when to stop the monitoring thread
        self._stop_event = threading.Event()
        self.monitor_thread = threading.Thread(target=self.monitor_resources, daemon=True)
        self.t_sleep=t_sleep

    def monitor_resources(self):
        while not self._stop_event.is_set():
            print(f"RAM Usage: {psutil.virtual_memory().percent}% | CPU Usage: {psutil.cpu_percent()}%")
            time.sleep(self.t_sleep)

    def start(self):
        # Start the daemon thread
        self.monitor_thread.start()

    def stop(self):
        # Signal the thread to stop and wait for it to finish
        self._stop_event.set()
        self.monitor_thread.join()  # Wait for the thread to terminate cleanly


def trimmed_mean(expr_mat,threshold=0.05):

    t_index=int(np.floor(expr_mat.shape[0]*(threshold/2)))

    trim_expr_mat=np.zeros((expr_mat.shape[0]-(2*t_index),expr_mat.shape[1]))

    for i in range(expr_mat.shape[1]):
        trim_expr_mat[:,i]=expr_mat[:,i][t_index:-t_index]

    return trim_expr_mat


expr_mat_wnt_trim=trimmed_mean(expr_mat_wnt_sc)


def optim_pearson(x,y):
    # Assumes inputs are DataFrames and computation is to be performed
    # pairwise between columns. We convert to arrays and reshape so calculation
    # is performed according to normal broadcasting rules along the last axis.
    x = np.asarray(x).T[:, np.newaxis, :]
    y = np.asarray(y).T
    n = x.shape[-1]

    # Compute Pearson correlation coefficient. We can't use `cov` or `corrcoef`
    # because they want to compute everything pairwise between rows of a
    # stacked x and y.
    xm = x.mean(axis=-1, keepdims=True)
    ym = y.mean(axis=-1, keepdims=True)
    cov = np.sum((x - xm) * (y - ym), axis=-1)/(n-1)
    sx = np.std(x, ddof=1, axis=-1)
    sy = np.std(y, ddof=1, axis=-1)
    rho = cov/(sx * sy)

    # Compute the two-sided p-values. See documentation of scipy.stats.pearsonr.
    ab = n/2 - 1
    x = (abs(rho) + 1)/2
    p = 2*(1-betainc(ab, ab, x))
    return rho, p


def chunkify_expr_mat(expr_mat,start=0,step=500):
    
    chunk_points = list(range(start,expr_mat.shape[1],step))+[expr_mat.shape[1]]
    
    for i in range(len(chunk_points)-1):
        yield chunk_points[i],chunk_points[i+1]


def generator_chunks(expr_mat,start=0,step=500):
    
    chunks = [chunk for chunk in chunkify_expr_mat(expr_mat=expr_mat,start=start,step=step)]

    for i,j in zip(*np.triu_indices(len(chunks))):
        chunk1=chunks[i]
        chunk2=chunks[j]
        if chunk1==chunk2:
            k = True
        else:
            k = False
        
        yield chunk1, chunk2, k


def compute_adj_mat(x,y,p_thresh,rho_thresh,chunks,k):

    rho, p = optim_pearson(x,y)

    p = np.where(p<=p_thresh,1,0)
    
    if k:
        np.fill_diagonal(p,0)
        
    rho*=p
    
    mask = np.where(abs(rho) >= rho_thresh,1,0)
    
    return rho * mask


def pool_compute_adj_mat(expr_mat,p_thresh=0.05,rho_thresh=0.7,t_sleep=2):

    adj_mat = np.zeros((expr_mat.shape[1],expr_mat.shape[1]))

    monitor = ResourceMonitor(t_sleep)
    monitor.start()

    results_dict = {}
    with ProcessPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(compute_adj_mat,expr_mat[:,chunk1[0]:chunk1[1]],expr_mat[:,chunk2[0]:chunk2[1]],p_thresh,rho_thresh,np.array([chunk1,chunk2]),k):np.array([chunk1,chunk2]) for chunk1, chunk2, k in generator_chunks(expr_mat,step=500)}

        for future in as_completed(futures):
            chunks = futures[future]
            try :         
                adj_mat[chunks[0][0]:chunks[0][1],chunks[1][0]:chunks[1][1]]=future.result()
            except Exception as e:
                print(f"Error in processing {chunks}: {e}")
                
    monitor.stop()
    adj_mat_trans = adj_mat.T
    adj_mat = np.where(adj_mat == 0, adj_mat_trans, adj_mat)
    return adj_mat


adj_mat_pearson = pool_compute_adj_mat(expr_mat=expr_mat_wnt_trim)


adj_mat_pearson_sum = abs(adj_mat_pearson).sum(axis=0)
pos = np.where(adj_mat_pearson_sum>0)[0].tolist()
genes_columns=expr_mat_wnt.columns.tolist()
genes = [genes_columns[i] for i in range(len(pos))]


path_network = '/home/thomas/Documents/git/medulloblastoma_cavalli_kaggle/data/network/'


adj_mat_pearson=adj_mat_pearson[pos,:][:,pos]
adj_mat_pearson_df=pd.DataFrame(data=adj_mat_pearson,columns=genes,index=genes)
adj_mat_pearson_df.to_csv(path_network+'adj_mat_pearson.csv')


import networkx as nx
import matplotlib.colors as mcolors
import matplotlib.cm as cm

class NetworkPearson:
    
    def __init__(self,adj_mat:np.ndarray=None,genes:list=None,_seed=42):
        self.adj_mat=adj_mat
        self.genes=genes
        self._seed=_seed
        self.graph=nx.Graph()

    def add_nodes(self):
        self.graph.add_nodes_from(self.genes)

    def add_edges(self):

        adj_mat_triu = np.triu(self.adj_mat,k=1)
        adj_mat_idx = np.where(adj_mat_triu!=0)

        for i,j in zip(*adj_mat_idx):
            node1=genes[i]
            node2=genes[j]
            if not self.graph.has_edge(node1,node2):
                self.graph.add_edge(node1,node2,weight=self.adj_mat[i,j])

    def get_weigths(self):
        return [self.graph[u][v]['weight'] for u,v in self.graph.edges()]
            
    
    


cmap = cm.seismic_r  # red -> white -> blue
norm = mcolors.Normalize(vmin=-1, vmax=1)
sm = cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

# Visualiser la colormap inversée
fig, ax = plt.subplots(figsize=(6, 1))
fig.colorbar(sm, cax=ax, orientation='horizontal')
ax.set_title('Colormap: Red → White → Blue')

plt.show()



np.round(adj_mat_pearson,decimals=2,out=adj_mat_pearson)


graph_pearson = NetworkPearson(adj_mat=adj_mat_pearson,genes=genes)
graph_pearson.add_nodes()
graph_pearson.add_edges()
weights=graph_pearson.get_weigths()
edges_colors=[cmap(norm(weight)) for weight in weights]


plt.figure(figsize=(12, 12))
pos = nx.spring_layout(graph_pearson.graph, seed=42, k=0.15)
nx.draw_networkx_nodes(graph_pearson.graph, pos, node_size=10, node_color='orange')
nx.draw_networkx_edges(graph_pearson.graph, pos, edge_color=edges_colors, alpha=0.5)
plt.axis('off')
plt.savefig(path_network + 'network_wnt_pearson.png')
plt.show()


adj_mat_pearson_edge = np.where(adj_mat_pearson!=0,1,0)

#plot degrees
fig, (ax1, ax2) = plt.subplots(2,1,figsize = (12, 12))
ax1.hist(adj_mat_pearson_edge.sum(axis=0),bins=32,edgecolor='black',color='white',density=False)
ax2.hist(adj_mat_pearson_edge.sum(axis=0),bins=32,edgecolor='black',color='white',density=True)
plt.show()


expr_mat_wnt_sc


fig, (ax1,ax2)  = plt.subplots(1,2,figsize=(9,4))
ax1.plot(expr_mat_wnt_sc[:,0],expr_mat_wnt_sc[:,54],'o',alpha=0.3, c='black')
ax1.set_ylabel('SPACA1')
ax1.set_xlabel('IFNA10')

ax2.plot(expr_mat_wnt_sc[:,0],expr_mat_wnt_sc[:,60],'o',alpha=0.3, c='black')
ax2.set_ylabel('OR6C2')
ax2.set_xlabel('IFNA10')

plt.show()



